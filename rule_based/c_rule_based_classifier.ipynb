{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-translator\n",
        "!pip install googletrans==4.0.0-rc1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYOMgK7-BDnA",
        "outputId": "21c03315-9598-45b7-ab5b-7bbb3ef77173"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.11.4-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.12.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2024.8.30)\n",
            "Downloading deep_translator-1.11.4-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.3/42.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.11.4\n",
            "Collecting googletrans==4.0.0-rc1\n",
            "  Downloading googletrans-4.0.0rc1.tar.gz (20 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting httpx==0.13.3 (from googletrans==4.0.0-rc1)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (2024.8.30)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hstspreload-2024.9.1-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans==4.0.0-rc1) (1.3.1)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting idna==2.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl.metadata (9.1 kB)\n",
            "Collecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl.metadata (8.1 kB)\n",
            "Collecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl.metadata (32 kB)\n",
            "Collecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==4.0.0-rc1)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Downloading hstspreload-2024.9.1-py3-none-any.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-4.0.0rc1-py3-none-any.whl size=17397 sha256=d141ffa406d06299131703a821bd366ba4d89a76d8ddb8084d6f8b6311d8a362\n",
            "  Stored in directory: /root/.cache/pip/wheels/c0/59/9f/7372f0cf70160fe61b528532e1a7c8498c4becd6bcffb022de\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.10\n",
            "    Uninstalling idna-3.10:\n",
            "      Successfully uninstalled idna-3.10\n",
            "Successfully installed chardet-3.0.4 googletrans-4.0.0rc1 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.9.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "# Corpus and Lemmatizers\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download resources\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Cn5AVf0pZ-b",
        "outputId": "dbb259e7-979c-4e6b-9298-082fcf828153",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------ Scatch and Test -------\n",
        "\n",
        "def scatch():\n",
        "  return\n",
        "\n",
        "scatch()"
      ],
      "metadata": {
        "id": "qLVhgCbYn8Lh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------ Main program -------\n",
        "#   IMPORTANT NOTE - Here we use pre-translated data from saved translated excel files (csv), as we don't want to translate here\n",
        "\n",
        "\n",
        "# In main we\n",
        "def main_by_language(lang= \"ru\", filename = \"_\"):\n",
        "  lang_dict = {\"ru\": \"Russian\", \"fi\": \"Finnish\", \"ja\": \"Japanese\"}\n",
        "  title = \"****************  \" + lang_dict[lang] +\"  ****************\"\n",
        "  print(title)\n",
        "  # Read file (use \"get_df_from_path(filepath)\" if the file is not yet uploaded)\n",
        "  df = pd.read_csv(filename)\n",
        "\n",
        "  # Make Predictios (and combine them into an overwritten df)\n",
        "  df = predict_and_update_df(df, threshold=2)\n",
        "\n",
        "  # Result Analysis\n",
        "  result_analysis(df)\n",
        "  for i in range(2):\n",
        "    print(\"----------------------------------------------------\")\n",
        "  print(\"\")\n",
        "  return df\n",
        "\n",
        "\n",
        "def main():\n",
        "  print(\"Evaluating Rule-Based Model\")\n",
        "  ru_df = main_by_language(lang= \"ru\", filename = \"ru_validation_translated.csv\")\n",
        "  fi_df = main_by_language(lang= \"fi\", filename = \"fi_validation_translated.csv\")\n",
        "  ja_df = main_by_language(lang= \"ja\", filename = \"ja_validation_translated.csv\")\n",
        "\n",
        "  all = pd.concat([ru_df, fi_df, ja_df])\n",
        "  print(\"****************  Overall Results  ****************\")\n",
        "  result_analysis(all)\n",
        "  return all\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "s3LGwICnEQkp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "56fe1b4e-8cad-4b6d-ca5f-a8023b371999"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating Rule-Based Model\n",
            "****************  Russian  ****************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.37      0.42      0.39       112\n",
            "        True       0.76      0.72      0.74       284\n",
            "\n",
            "    accuracy                           0.64       396\n",
            "   macro avg       0.57      0.57      0.57       396\n",
            "weighted avg       0.65      0.64      0.64       396\n",
            "\n",
            "Total of 396 Questions\n",
            "Count of answerable questions:  284 = % 71.71717171717171\n",
            "Count of correctly predicted answerable questions:  205 = % 72.1830985915493\n",
            "Count of non-answerable questions:  112 = % 28.28282828282828\n",
            "Count of correctly predicted non-answerable questions:  47 = % 41.964285714285715\n",
            "---- TPR:  0.721830985915493\n",
            "---- TNR:  0.41964285714285715\n",
            "Weighted Accuracy (recall):  0.5707369215291751\n",
            "Overall Accuracy: 0.6363636363636364\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "\n",
            "****************  Finnish  ****************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.23      0.20      0.21       148\n",
            "        True       0.70      0.74      0.72       380\n",
            "\n",
            "    accuracy                           0.59       528\n",
            "   macro avg       0.47      0.47      0.47       528\n",
            "weighted avg       0.57      0.59      0.58       528\n",
            "\n",
            "Total of 528 Questions\n",
            "Count of answerable questions:  380 = % 71.96969696969697\n",
            "Count of correctly predicted answerable questions:  283 = % 74.47368421052632\n",
            "Count of non-answerable questions:  148 = % 28.030303030303028\n",
            "Count of correctly predicted non-answerable questions:  29 = % 19.594594594594593\n",
            "---- TPR:  0.7447368421052631\n",
            "---- TNR:  0.19594594594594594\n",
            "Weighted Accuracy (recall):  0.47034139402560454\n",
            "Overall Accuracy: 0.5909090909090909\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "\n",
            "****************  Japanese  ****************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.41      0.33      0.36       169\n",
            "        True       0.65      0.73      0.69       287\n",
            "\n",
            "    accuracy                           0.58       456\n",
            "   macro avg       0.53      0.53      0.52       456\n",
            "weighted avg       0.56      0.58      0.57       456\n",
            "\n",
            "Total of 456 Questions\n",
            "Count of answerable questions:  287 = % 62.93859649122807\n",
            "Count of correctly predicted answerable questions:  209 = % 72.82229965156795\n",
            "Count of non-answerable questions:  169 = % 37.06140350877193\n",
            "Count of correctly predicted non-answerable questions:  55 = % 32.544378698224854\n",
            "---- TPR:  0.7282229965156795\n",
            "---- TNR:  0.3254437869822485\n",
            "Weighted Accuracy (recall):  0.526833391748964\n",
            "Overall Accuracy: 0.5789473684210527\n",
            "----------------------------------------------------\n",
            "----------------------------------------------------\n",
            "\n",
            "****************  Overall Results  ****************\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       0.34      0.31      0.32       429\n",
            "        True       0.70      0.73      0.72       951\n",
            "\n",
            "    accuracy                           0.60      1380\n",
            "   macro avg       0.52      0.52      0.52      1380\n",
            "weighted avg       0.59      0.60      0.59      1380\n",
            "\n",
            "Total of 1380 Questions\n",
            "Count of answerable questions:  951 = % 68.91304347826087\n",
            "Count of correctly predicted answerable questions:  697 = % 73.29127234490011\n",
            "Count of non-answerable questions:  429 = % 31.08695652173913\n",
            "Count of correctly predicted non-answerable questions:  131 = % 30.536130536130536\n",
            "---- TPR:  0.732912723449001\n",
            "---- TNR:  0.30536130536130535\n",
            "Weighted Accuracy (recall):  0.5191370144051533\n",
            "Overall Accuracy: 0.6\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Unnamed: 0                                           question  \\\n",
              "0           379               Where is the Ig Nobel Prize awarded?   \n",
              "1           380  In which theater did Sergei Ervandovich Kurgin...   \n",
              "2           381           Was François Auguste René Rodin married?   \n",
              "3           382  How long did the battle for the city of Deir e...   \n",
              "4           383  How many hero statues are there in Heroes' Squ...   \n",
              "..          ...                                                ...   \n",
              "451        2823           What was the first song released by PYG?   \n",
              "452        2824  What is the title of the remake of Queer as Folk?   \n",
              "453        2825  What is the name of the hometown of the main c...   \n",
              "454        2826  Which African country had the highest GDP in 2...   \n",
              "455        2827  Who were the first Western Europeans to reach ...   \n",
              "\n",
              "                                               context lang  answerable  \\\n",
              "0    Organized by the scientific humor magazine, th...   ru        True   \n",
              "1    Kurginyan was a member of the commission on ne...   ru        True   \n",
              "2    From the unexpected realism of his first major...   ru       False   \n",
              "3    The battle for Deir ez-Zor was a conflict betw...   ru        True   \n",
              "4    In Budapest, Hungary, the Heroes' Square, bett...   ru        True   \n",
              "..                                                 ...  ...         ...   \n",
              "451  (Ittoku Kishibe) and Kenji Sawada gathered. Si...   ja       False   \n",
              "452  The profit margin is thin due to the fact that...   ja       False   \n",
              "453  is a young student who has lost all her memori...   ja       False   \n",
              "454  In 2017, the country fell from the second most...   ja       False   \n",
              "455  and various poultry, and lived in close proxim...   ja       False   \n",
              "\n",
              "     answer_start                               answer  prediction  \n",
              "0             161  Sanders Theater, Harvard University        True  \n",
              "1             244               his own Theatre Studio        True  \n",
              "2              -1                                   no        True  \n",
              "3             219                          three years        True  \n",
              "4             186                                seven        True  \n",
              "..            ...                                  ...         ...  \n",
              "451            -1                    flower, sun, rain       False  \n",
              "452            -1                            queer eye       False  \n",
              "453            -1                          Masala Town        True  \n",
              "454            -1                              Nigeria        True  \n",
              "455            -1                           John Cabot        True  \n",
              "\n",
              "[1380 rows x 8 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-11b38f15-2e7c-4b69-8fe6-f2eec5ab3830\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>question</th>\n",
              "      <th>context</th>\n",
              "      <th>lang</th>\n",
              "      <th>answerable</th>\n",
              "      <th>answer_start</th>\n",
              "      <th>answer</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>379</td>\n",
              "      <td>Where is the Ig Nobel Prize awarded?</td>\n",
              "      <td>Organized by the scientific humor magazine, th...</td>\n",
              "      <td>ru</td>\n",
              "      <td>True</td>\n",
              "      <td>161</td>\n",
              "      <td>Sanders Theater, Harvard University</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>380</td>\n",
              "      <td>In which theater did Sergei Ervandovich Kurgin...</td>\n",
              "      <td>Kurginyan was a member of the commission on ne...</td>\n",
              "      <td>ru</td>\n",
              "      <td>True</td>\n",
              "      <td>244</td>\n",
              "      <td>his own Theatre Studio</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>381</td>\n",
              "      <td>Was François Auguste René Rodin married?</td>\n",
              "      <td>From the unexpected realism of his first major...</td>\n",
              "      <td>ru</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>no</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>382</td>\n",
              "      <td>How long did the battle for the city of Deir e...</td>\n",
              "      <td>The battle for Deir ez-Zor was a conflict betw...</td>\n",
              "      <td>ru</td>\n",
              "      <td>True</td>\n",
              "      <td>219</td>\n",
              "      <td>three years</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>383</td>\n",
              "      <td>How many hero statues are there in Heroes' Squ...</td>\n",
              "      <td>In Budapest, Hungary, the Heroes' Square, bett...</td>\n",
              "      <td>ru</td>\n",
              "      <td>True</td>\n",
              "      <td>186</td>\n",
              "      <td>seven</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>451</th>\n",
              "      <td>2823</td>\n",
              "      <td>What was the first song released by PYG?</td>\n",
              "      <td>(Ittoku Kishibe) and Kenji Sawada gathered. Si...</td>\n",
              "      <td>ja</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>flower, sun, rain</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>452</th>\n",
              "      <td>2824</td>\n",
              "      <td>What is the title of the remake of Queer as Folk?</td>\n",
              "      <td>The profit margin is thin due to the fact that...</td>\n",
              "      <td>ja</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>queer eye</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>453</th>\n",
              "      <td>2825</td>\n",
              "      <td>What is the name of the hometown of the main c...</td>\n",
              "      <td>is a young student who has lost all her memori...</td>\n",
              "      <td>ja</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>Masala Town</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>454</th>\n",
              "      <td>2826</td>\n",
              "      <td>Which African country had the highest GDP in 2...</td>\n",
              "      <td>In 2017, the country fell from the second most...</td>\n",
              "      <td>ja</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>Nigeria</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>455</th>\n",
              "      <td>2827</td>\n",
              "      <td>Who were the first Western Europeans to reach ...</td>\n",
              "      <td>and various poultry, and lived in close proxim...</td>\n",
              "      <td>ja</td>\n",
              "      <td>False</td>\n",
              "      <td>-1</td>\n",
              "      <td>John Cabot</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1380 rows × 8 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-11b38f15-2e7c-4b69-8fe6-f2eec5ab3830')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-11b38f15-2e7c-4b69-8fe6-f2eec5ab3830 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-11b38f15-2e7c-4b69-8fe6-f2eec5ab3830');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-d63855bd-5910-406e-8ede-46d73b085df7\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d63855bd-5910-406e-8ede-46d73b085df7')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-d63855bd-5910-406e-8ede-46d73b085df7 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"main()\",\n  \"rows\": 1380,\n  \"fields\": [\n    {\n      \"column\": \"Unnamed: 0\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 768,\n        \"min\": 311,\n        \"max\": 2927,\n        \"num_unique_values\": 1380,\n        \"samples\": [\n          2909,\n          1694,\n          503\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"question\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 946,\n        \"samples\": [\n          \"Where was Auto Union's first headquarters located?\",\n          \"When was Sir William Rowan Hamilton born?\",\n          \"In Buddhism, does attaining enlightenment make you a Buddha?\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"context\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1366,\n        \"samples\": [\n          \"The execution time [or storage space] of 10 is less than or equal to formula_11. where formula_12 represents the length of the string x. Even if the above definition is changed for each of the following items, the inductive enumeration language does not change, and the effect on the time complexity and space complexity is small. For this reason, the details of the definition of a Turing machine vary in the literature.\\nfiner spatial complexity\",\n          \"The sitcom format was born in January 1926 with the initial broadcast of \\\"Sam 'n' Henry\\\" on WGN radio in Chicago, Illinois. The 15-minute daily program was revamped in 1928, moved to another station, renamed \\\"Amos 'n' Andy\\\", and became one of the most successful sitcoms of the period. It was also one of the earliest examples of radio syndication. Like many radio programs of the time, the two programs continued the American entertainment traditions of vaudeville and the minstrel show.\",\n          \"Malkin began his career with his hometown club Metallurg Magnitogorsk, playing for their junior and senior teams. He was then selected second overall in the 2004 NHL Draft by the Pittsburgh Penguins, though an international transfer dispute delayed the start of his NHL career until 2006. After his first season with the Penguins Malkin was awarded the Calder Memorial Trophy as the NHL's best rookie. In his second season, he helped lead Pittsburgh to the 2008 Stanley Cup Final and was a runner-up for the Hart Memorial Trophy, awarded to the NHL's most valuable player during the regular season. The following season saw Malkin win the Art Ross Trophy, awarded to the top-scorer in the NHL and again place second for the Hart Trophy. He and the Penguins again reached the Stanley Cup Final, winning the Stanley Cup championship this time around. Malkin was awarded the Conn Smythe Trophy as most valuable player of the playoffs. In 2012, Malkin was awarded the Hart Trophy and Ted Lindsay Award, awarded to the best player as voted on by the players, after winning the Art Ross Trophy for the second time; his 12-point lead was the largest margin of victory since 1999.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"lang\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"ru\",\n          \"fi\",\n          \"ja\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answerable\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer_start\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 196,\n        \"min\": -1,\n        \"max\": 2027,\n        \"num_unique_values\": 398,\n        \"samples\": [\n          297,\n          449\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"answer\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 934,\n        \"samples\": [\n          \"1985\",\n          \"Svetlana Bondarchuk\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"prediction\",\n      \"properties\": {\n        \"dtype\": \"boolean\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          false,\n          true\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Rule Based Decision:\n",
        "Idea: look for keywords and their spaces in the context. If there are found (any? at least 2?..) then answerable.\n",
        "\n",
        "\n",
        "Let wordspace(word) = {w' | w' and word has the same \"root\"}\n",
        "<br>\n",
        "For Example: wordspace(Green) = {Green, Greener, Greenest..}\n",
        "\n",
        "Algorithm1 (less efficient)\n",
        "1. Extract words from question -> bag_of_words\n",
        "2. Filter out stop words\n",
        "3. Expand bag_of_words to include lemmatizations\n",
        "3. Make both bag_of_words and context lower-case.\n",
        "4. Count appearaces of words in context **sentence** (i.e. \"german\" will be found in \"germanistic\")\n",
        "\n",
        "Algorithm 2:\n",
        "1. Extract words from question -> bag_of_words\n",
        "2. Filter out stop words\n",
        "3. **Reduce** i.e. **Lemmatize** both the words_in_context and bag_of_words\n",
        "4. count..\n",
        "\n"
      ],
      "metadata": {
        "id": "t4rWQUqED2Dw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---- Utility and Translation Functions ----\n",
        "\n",
        "def get_train_df():\n",
        "  splits = {'train': 'train.parquet', 'validation': 'validation.parquet'}\n",
        "  df = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"train\"])\n",
        "  return df\n",
        "\n",
        "\n",
        "\n",
        "def get_validation_df():\n",
        "  splits = {'train': 'train.parquet', 'validation': 'validation.parquet'}\n",
        "  df = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"validation\"])\n",
        "  return df\n",
        "\n",
        "# Takes the dataframe, language and maxmimum count and returns only the relevant results\n",
        "def get_translated_df_by_lang(df_, lang, batch_size=(-1)):\n",
        "\n",
        "  # Translators Import:\n",
        "  from googletrans import Translator\n",
        "  from deep_translator import GoogleTranslator\n",
        "\n",
        "  translator = GoogleTranslator(source=lang, target='en')\n",
        "  df = (df_[df_[\"lang\"] == lang]).copy()\n",
        "  if (batch_size == -1):\n",
        "    batch_size = len(df)\n",
        "  print(\"Number of questions to translate (batch_size): \", batch_size)\n",
        "  df = df[:batch_size]\n",
        "\n",
        "  questions = df['question']\n",
        "  questions_translated = [translator.translate(q, src= lang, dest='en') for q in questions ]\n",
        "  df['question'] = questions_translated\n",
        "  return df\n",
        "\n",
        "# ---- Import ----\n",
        "def get_df_from_path(path):\n",
        "  from google.colab import files\n",
        "  uploaded = files.upload()\n",
        "  df = pd.read_csv(path)\n",
        "  return df\n",
        "\n",
        "\n",
        "# ---- Translation and Saving -----\n",
        "\n",
        "def save_translated(df, lang = \"ru\", set_type = \"validation\", batch_num = 0):\n",
        "  filename = lang + \"_\" + set_type + \"_batch_\" + str(batch_num) + \".csv\"\n",
        "  df.to_csv(filename)\n",
        "  from google.colab import files\n",
        "  files.download(filename)\n",
        "\n",
        "def translate_and_save(lang = \"ru\", set_type = \"validation\", batch_size = 500):\n",
        "  df = get_validation_df() if set_type == \"validation\" else get_train_df()\n",
        "  if (batch_size > len(df[df[\"lang\"] == lang])):\n",
        "    df_translated = get_translated_df_by_lang(df, lang)\n",
        "    save_translated(df_translated, lang=lang, set_type=set_type)\n",
        "    return\n",
        "  batches = [df[i:i+batch_size] for i in range(0, df.shape[0], batch_size)]\n",
        "  for i, batch in enumerate(batches):\n",
        "    df_translated = get_translated_df_by_lang(df = batch, lang = lang, batch_size=batch_size)\n",
        "    save_translated(df_translated, lang=lang, set_type=set_type, batch_num=i)\n"
      ],
      "metadata": {
        "id": "iLztByKyGP7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WrpdOylm-OC"
      },
      "outputs": [],
      "source": [
        "# ---- NLP Helper Functions ----\n",
        "#   Includes:\n",
        "#     tokenize_and_filter_stop_words(sentence)\n",
        "#     count_words_in_set(question_words, context_words)\n",
        "#     count_words_in_sentence(question_words, sentence)\n",
        "#     lemmatize_words(words)\n",
        "\n",
        "\n",
        "# Tokenizes (nltk) a text, filters out (english) stop words, and returns a set of the left out *lowercase* words*\n",
        "def tokenize_and_filter_stop_words(sentence):\n",
        "    from string import punctuation\n",
        "\n",
        "    # Tokenize the sentence\n",
        "    tokens = nltk.tokenize.word_tokenize(sentence)\n",
        "\n",
        "    # Get stop words\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "    # Filter out punctuation and stop words\n",
        "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word not in punctuation]\n",
        "\n",
        "    # make all lower case and make a set\n",
        "    filtered_tokens = set([word.lower() for word in filtered_tokens])\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "\n",
        "# Counts\n",
        "def count_words_in_set(question_words, context_words):\n",
        "  context_words = set([w.lower() for w in context_words])\n",
        "  # Count how many words from words_list are found in the sentence\n",
        "\n",
        "  true_false_words = [(word.lower() in context_words) for word in question_words]\n",
        "\n",
        "  count = sum(1 for word in question_words if word.lower() in context_words)\n",
        "  return count\n",
        "\n",
        "\n",
        "def count_words_in_sentence(question_words, sentence):\n",
        "  true_false_words = [(word.lower() in sentence) for word in question_words]\n",
        "\n",
        "  count = sum(1 for word in question_words if word.lower() in sentence)\n",
        "  return count\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# POS tag each word in words , and then lemmatize it, and append lemma to words\n",
        "def lemmatize_words(words):\n",
        "\n",
        "  # - Helper Function\n",
        "  # ---   Mapping NLTK POS tags to WordNet POS tags\n",
        "  def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "    # - End of Helper Function\n",
        "\n",
        "  #make sure words are a list and not set:\n",
        "  words = list(words)\n",
        "\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_words = []\n",
        "\n",
        "  # Get the POS tags for the words\n",
        "  pos_tags = nltk.pos_tag(words)\n",
        "\n",
        "  # Lemmatize\n",
        "  for word, tag in pos_tags:\n",
        "    wordnet_pos = get_wordnet_pos(tag) or wordnet.NOUN  # Default to NOUN if POS tag is not found\n",
        "    lemmatized_words.append(lemmatizer.lemmatize(word, pos=wordnet_pos))\n",
        "  all_words = lemmatized_words + words\n",
        "  return set(all_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction Function :\n",
        "\n",
        "# predict_single_set_context :\n",
        "# Counts appearences of lemmatized question words in lemmatized context word **set**\n",
        "def predict_single_set_context(df_row, threshold = 1):\n",
        "  question= df_row['question']\n",
        "  context = df_row['context']\n",
        "  words_sets = [[], []]\n",
        "  q_words = lemmatize_words(tokenize_and_filter_stop_words(question))\n",
        "  c_words = lemmatize_words(tokenize_and_filter_stop_words(context))\n",
        "  count = count_words_in_set(q_words, c_words)\n",
        "  if (count >= threshold):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "# predict_single_set_context :\n",
        "# Counts appearences of lemmatized question words in lower-cased context **sentence\n",
        "def predict_single(df_row, threshold = 1):\n",
        "  question= df_row['question']\n",
        "  context = df_row['context'].lower()\n",
        "  words_sets = [[], []]\n",
        "  q_words = lemmatize_words(tokenize_and_filter_stop_words(question))\n",
        "\n",
        "  count = count_words_in_sentence(q_words, context)\n",
        "  if (count >= threshold):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "## For debugging\n",
        "def predict_single_parse(df_row, threshold = 1):\n",
        "  question= df_row['question']\n",
        "  context = df_row['context']\n",
        "  words_sets = [[], []]\n",
        "  q_words = lemmatize_words(tokenize_and_filter_stop_words(question))\n",
        "  c_words = tokenize_and_filter_stop_words(context)\n",
        "  # not tokenizing\n",
        "\n",
        "  print(\"Question words:\\n\", q_words)\n",
        "  print(\"Context words:\\n\", c_words)\n",
        "  count = count_words_in_set(q_words, c_words)\n",
        "  count = count_words_in_sentence(q_words, context)\n",
        "  print(\"Count: \", count)\n",
        "  if (count >= threshold):\n",
        "    return True\n",
        "  return False\n",
        "\n",
        "def predict_many(df, threshold):\n",
        "  predictions = []\n",
        "  for i in range(len(df)):\n",
        "    predictions.append(predict_single(df.iloc[i], threshold))\n",
        "  return predictions\n",
        "\n",
        "def predict_and_update_df(df, threshold = 2):\n",
        "  ru_predictions = predict_many(df, threshold=threshold)\n",
        "  df2 = df.assign(prediction = ru_predictions)\n",
        "  df2.drop([\"answer_inlang\"], axis=1, inplace=True)\n",
        "  return df2\n",
        "\n",
        "\n",
        "def print_comparison(ru_predictions, ru_real_label):\n",
        "  for i in range(len(ru_predictions)):\n",
        "    pre_str = \"Yes\" if ru_predictions[i] else \"No\"\n",
        "    print(\"Questions %d -- Preciction: \" % (i) + pre_str + \", Expected: \", ru_real_label.iloc[i])\n",
        "  return\n",
        "\n"
      ],
      "metadata": {
        "id": "9-fZmsXJJRsb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Result Analysis Functions:\n",
        "from sklearn import metrics\n",
        "\n",
        "\n",
        "def result_analysis(df):\n",
        "\n",
        "  # Classification Report\n",
        "  print(metrics.classification_report(df['answerable'], df['prediction']))\n",
        "\n",
        "  df_size = df.shape[0]\n",
        "\n",
        "  print(\"Total of %d Questions\" % (df_size))\n",
        "\n",
        "  # TP, FP in details:\n",
        "  ## how many true and how many out of them were predicted correctly?\n",
        "  true_count = len(df[df['answerable'] == True])\n",
        "  print(\"Count of answerable questions: \", true_count, \"= %\", (true_count/df_size)*100)\n",
        "  correct_true_count = len(df[(df['answerable'] == True) & (df['prediction'] == True)])\n",
        "  print(\"Count of correctly predicted answerable questions: \", correct_true_count,  \"= %\", (correct_true_count/true_count)*100)\n",
        "\n",
        "    ## how many true and how many out of them were predicted correctly?\n",
        "  false_count = len(df[df['answerable'] == False])\n",
        "  print(\"Count of non-answerable questions: \", false_count, \"= %\", (false_count/df_size)*100)\n",
        "  correct_false_count = len(df[(df['answerable'] == False) & (df['prediction'] == False)])\n",
        "  print(\"Count of correctly predicted non-answerable questions: \", correct_false_count, \"= %\", (correct_false_count/false_count)*100)\n",
        "\n",
        "\n",
        "  con_matrix = metrics.confusion_matrix(df['answerable'], df['prediction'], normalize='true')\n",
        "  tnr, tpr = con_matrix.diagonal()\n",
        "\n",
        "\n",
        "  # Accuracy Metrics\n",
        "  accuracy = len(df[df['answerable'] == df['prediction']]) / len(df)\n",
        "  weighted_accuracy = tpr * 0.5 + tnr * 0.5\n",
        "\n",
        "\n",
        "\n",
        "  print(\"---- TPR: \", tpr)\n",
        "  print(\"---- TNR: \", tnr)\n",
        "  print(\"Weighted Accuracy (recall): \", weighted_accuracy)\n",
        "  print(\"Overall Accuracy:\", accuracy)\n",
        "  return\n"
      ],
      "metadata": {
        "id": "lZ58g9LRQAPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ****** False Positives Analysis *******\n",
        "\n",
        "\"\"\" ----- Observatios -----:\n",
        "For many questions, the ground-truth labeling as 'false' can be debated upon\n",
        "\n",
        "  1. For example, question 7239:\n",
        "     \"Greece lost in the First World War?\"\n",
        "     Context begins with: \"As Greece emerged victorious from World War I,\"\n",
        "\n",
        "  2. Question 7240: \"In Iran, women have equal rights with men?\"\n",
        "     In context: \"The World Economic Forum’s 2017 Global Gender Gap Report ranked Iran 140 out of 144 countries for gender parity.\"\n",
        "\n",
        "  3. Questions 7295: \"Lev Trotsky was born in a wealthy family? \"\n",
        "     In Context: \"Leon Trotsky was born Lev Davidovich Bronstein on 7 November 1879,\n",
        "      the fifth child of a Ukrainian-Jewish family of wealthy farmers in Yanovka or Yanivka\"\n",
        "\n",
        "  4. Questions 7292 (Ukraines don't need visa for russia.. (back then))..\n",
        "\n",
        "\n",
        "Many More other examples - therefore it seems the labeling for the unanswerable questions\n",
        "  is VERY unaccurate!!\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "0aSZHO0XrdNb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zXruKmczKLj3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "outputId": "e3b2cc11-f412-48c3-9509-8882d56a079b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c42a15b2c7cf>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    }
  ]
}