{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YBEDhXkSVlF-"
      },
      "source": [
        "**Assignment 2 - ngram models**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWFb9HUuU_hF",
        "outputId": "a04666f9-e6f0-4b33-b060-c5bd79c14e8d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "splits = {'train': 'train.parquet', 'validation': 'validation.parquet'}\n",
        "df = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"train\"])\n",
        "df_validation = pd.read_parquet(\"hf://datasets/coastalcph/tydi_xor_rc/\" + splits[\"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fQHu_rIfVrGJ"
      },
      "outputs": [],
      "source": [
        "finnish_df = df[df['lang'] == 'fi']['question']\n",
        "japanese_df = df[df['lang'] == 'ja']['question']\n",
        "russian_df = df[df['lang'] == 'ru']['question']\n",
        "english_df = df['context']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9CxIU2-6V8z0"
      },
      "outputs": [],
      "source": [
        "finnish_validation_df = df_validation[df_validation['lang'] == 'fi']['question']\n",
        "japanese_validation_df = df_validation[df_validation['lang'] == 'ja']['question']\n",
        "russian_validation_df = df_validation[df_validation['lang'] == 'ru']['question']\n",
        "english_validation_df = df_validation['context']\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "id": "xOs8GVfRV-oM",
        "outputId": "9616f3a9-fb05-4850-d053-7b60b78bb605"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>context</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Portland is the largest city in the U.S. state...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The word cholera is from \"kholera\" from χολή \"...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Since it became widespread in the 19th century...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>World War I occurred from 1914 to 1918. In ter...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>World War I (often abbreviated as WWI or WW1),...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3023</th>\n",
              "      <td>Reyyalagadda is a village belonging to Gangara...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3024</th>\n",
              "      <td>Boothumillipadu is a village in Gannavaram man...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3025</th>\n",
              "      <td>Mallavemula is a village belonging to Chagalam...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3026</th>\n",
              "      <td>Andria Urushadze (; born April 25, 1968) is a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3027</th>\n",
              "      <td>Guntur district is 11,391 sq. km. Spread over ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3028 rows × 1 columns</p>\n",
              "</div><br><label><b>dtype:</b> object</label>"
            ],
            "text/plain": [
              "0       Portland is the largest city in the U.S. state...\n",
              "1       The word cholera is from \"kholera\" from χολή \"...\n",
              "2       Since it became widespread in the 19th century...\n",
              "3       World War I occurred from 1914 to 1918. In ter...\n",
              "4       World War I (often abbreviated as WWI or WW1),...\n",
              "                              ...                        \n",
              "3023    Reyyalagadda is a village belonging to Gangara...\n",
              "3024    Boothumillipadu is a village in Gannavaram man...\n",
              "3025    Mallavemula is a village belonging to Chagalam...\n",
              "3026    Andria Urushadze (; born April 25, 1968) is a ...\n",
              "3027    Guntur district is 11,391 sq. km. Spread over ...\n",
              "Name: context, Length: 3028, dtype: object"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "english_validation_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6QZKUbHqZvw_",
        "outputId": "215a6ee1-5641-4ca3-bc1e-385bf3933ae3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "a8c38f1298694f179497fd7414a69fe4",
              "pip_warning": {
                "packages": [
                  "pyarrow"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install transformers datasets torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Opum4CqHMgHr",
        "outputId": "0e0b69ef-1743-4434-e38d-449d606e07a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7XubvPv8IbtB"
      },
      "source": [
        "Attempt with NLTK"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbEFf3nMFiZp",
        "outputId": "0ff2861a-8faa-4a59-ee39-e1a14c0d50e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
        "from nltk.lm import Laplace, KneserNeyInterpolated, WittenBellInterpolated, Lidstone, MLE\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from functools import partial\n",
        "from nltk.util import everygrams, flatten\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "W9tmnRejl0Ga"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "detokenize = TreebankWordDetokenizer().detokenize\n",
        "\n",
        "def generate_sent(model, num_words, random_seed=42):\n",
        "    \"\"\"\n",
        "    :param model: An ngram language model from `nltk.lm.model`.\n",
        "    :param num_words: Max no. of words to generate.\n",
        "    :param random_seed: Seed value for random.\n",
        "    \"\"\"\n",
        "    content = []\n",
        "    for token in model.generate(num_words, random_seed=random_seed):\n",
        "        if token == '<s>':\n",
        "            continue\n",
        "        if token == '</s>':\n",
        "            break\n",
        "        content.append(token)\n",
        "    return detokenize(content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "lyGccXt3l0Ga"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity_Laplace(train_data, test_data, n=5):\n",
        "    # Tokenize the data\n",
        "    train_data_tokenized = [list(map(str.lower, word_tokenize(sent))) for sent in train_data]\n",
        "    test_data_tokenized = [list(map(str.lower, word_tokenize(sent))) for sent in test_data]\n",
        "\n",
        "    # Prepare the data for the language model\n",
        "    train_data, padded_vocab = padded_everygram_pipeline(n, train_data_tokenized)\n",
        "\n",
        "    # Train the model with Laplace smoothing\n",
        "    model = Laplace(n)\n",
        "    model.fit(train_data, padded_vocab)\n",
        "\n",
        "    # Prepare the test data for perplexity calculation\n",
        "    test_data_ngrams, _ = padded_everygram_pipeline(n, test_data_tokenized)\n",
        "\n",
        "    # Convert the test_data generator to a list of n-grams\n",
        "    test_data_ngrams_list = [list(ngrams) for ngrams in test_data_ngrams]\n",
        "\n",
        "    # Flatten the list of lists into a single list of n-grams\n",
        "    flattened_test_data_ngrams = [ngram for sublist in test_data_ngrams_list for ngram in sublist]\n",
        "\n",
        "    # Calculate perplexity\n",
        "    perplexity = model.perplexity(flattened_test_data_ngrams)\n",
        "\n",
        "    # Get the size of the vocabulary\n",
        "    vocab_size = len(model.vocab)\n",
        "\n",
        "    return perplexity, vocab_size, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0vmHKUuJl0Ga"
      },
      "outputs": [],
      "source": [
        "#edit to n=5 for the second model\n",
        "ngramsize=3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pu_Szs-Gl0Ga",
        "outputId": "566c4fa7-5497-4c2f-a671-d04d389889aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finnish Perplexity: 179.35638578310903, Vocabulary Size: 4421\n",
            "Japanese Perplexity: 81.71054095888772, Vocabulary Size: 2400\n",
            "Russian Perplexity: 275.1710778106379, Vocabulary Size: 4867\n",
            "English Perplexity: 4445.265987464632, Vocabulary Size: 83494\n"
          ]
        }
      ],
      "source": [
        "# Calculate perplexity for each dataset\n",
        "finnish_perplexity, finnish_vocab_size, finnish_model = calculate_perplexity_Laplace(finnish_df, finnish_df, n=ngramsize)\n",
        "print(f'Finnish Perplexity: {finnish_perplexity}, Vocabulary Size: {finnish_vocab_size}')\n",
        "\n",
        "japanese_perplexity, japanese_vocab_size, japanese_model = calculate_perplexity_Laplace(japanese_df, japanese_df, n=ngramsize)\n",
        "print(f'Japanese Perplexity: {japanese_perplexity}, Vocabulary Size: {japanese_vocab_size}')\n",
        "\n",
        "russian_perplexity, russian_vocab_size, russian_model = calculate_perplexity_Laplace(russian_df, russian_df, n=ngramsize)\n",
        "print(f'Russian Perplexity: {russian_perplexity}, Vocabulary Size: {russian_vocab_size}')\n",
        "\n",
        "english_perplexity, english_vocab_size, english_model = calculate_perplexity_Laplace(english_df, english_df, n=ngramsize)\n",
        "print(f'English Perplexity: {english_perplexity}, Vocabulary Size: {english_vocab_size}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMqLUZOtl0Gb"
      },
      "source": [
        "Experimentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "w4FOIMval0Gb",
        "outputId": "844c7c03-8385-4f32-e4c9-fec4cfdb327c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"of agreements are, however neither of which east godavari district or paschima godavari jilla is one of patrick brontë's death in 80% of carbon dioxide in the united states air force), and doubts as to the public on november 22, 1876 (at right\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "generate_sent(english_model, num_words=50, random_seed=69)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ff9k-Z-3l0Gb",
        "outputId": "91aca712-1e47-4486-f47a-809d4750ce41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5.2235851136182e-07"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "english_model.score('What')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8lwg2J1l0Gb"
      },
      "source": [
        "KneserNeyInterpolated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ABXES95yl0Gb"
      },
      "outputs": [],
      "source": [
        "def calculate_perplexity_WrittenBell(train_data, test_data, n=5):\n",
        "    # Tokenize the data\n",
        "    train_data_tokenized = [list(map(str.lower, word_tokenize(sent))) for sent in train_data]\n",
        "    test_data_tokenized = [list(map(str.lower, word_tokenize(sent))) for sent in test_data]\n",
        "\n",
        "    # Prepare the data for the language model\n",
        "    train_data, padded_vocab = padded_everygram_pipeline(n, train_data_tokenized)\n",
        "\n",
        "    # Train the model with Laplace smoothing\n",
        "    model = WittenBellInterpolated(n)\n",
        "    model.fit(train_data, padded_vocab)\n",
        "\n",
        "    # Prepare the test data for perplexity calculation\n",
        "    test_data_ngrams, _ = padded_everygram_pipeline(n, test_data_tokenized)\n",
        "\n",
        "    # Convert the test_data generator to a list of n-grams\n",
        "    test_data_ngrams_list = [list(ngrams) for ngrams in test_data_ngrams]\n",
        "\n",
        "    # Flatten the list of lists into a single list of n-grams\n",
        "    flattened_test_data_ngrams = [ngram for sublist in test_data_ngrams_list for ngram in sublist]\n",
        "\n",
        "    # Calculate perplexity\n",
        "    perplexity = model.perplexity(flattened_test_data_ngrams)\n",
        "\n",
        "    # Get the size of the vocabulary\n",
        "    vocab_size = len(model.vocab)\n",
        "\n",
        "    return perplexity, vocab_size, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O05TzAk3l0Gc",
        "outputId": "febc3afb-3ee3-4d75-dbd6-400a93f2f408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finnish Perplexity: 13.98932372188909, Vocabulary Size: 4421\n",
            "Japanese Perplexity: 13.469293755464399, Vocabulary Size: 2400\n",
            "Russian Perplexity: 16.73508743543924, Vocabulary Size: 4867\n"
          ]
        }
      ],
      "source": [
        "# Calculate perplexity for each dataset\n",
        "finnish_perplexity, finnish_vocab_size, finnish_model = calculate_perplexity_WrittenBell(finnish_df, finnish_df, n=ngramsize)\n",
        "print(f'Finnish Perplexity: {finnish_perplexity}, Vocabulary Size: {finnish_vocab_size}')\n",
        "\n",
        "japanese_perplexity, japanese_vocab_size, japanese_model = calculate_perplexity_WrittenBell(japanese_df, japanese_df, n=ngramsize)\n",
        "print(f'Japanese Perplexity: {japanese_perplexity}, Vocabulary Size: {japanese_vocab_size}')\n",
        "\n",
        "russian_perplexity, russian_vocab_size, russian_model = calculate_perplexity_WrittenBell(russian_df, russian_df, n=ngramsize)\n",
        "print(f'Russian Perplexity: {russian_perplexity}, Vocabulary Size: {russian_vocab_size}')\n",
        "\n",
        "english_perplexity, english_vocab_size, english_model = calculate_perplexity_WrittenBell(english_df, english_df, n=ngramsize)\n",
        "print(f'English Perplexity: {english_perplexity}, Vocabulary Size: {english_vocab_size}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ChJQBpwdl0Gc"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
